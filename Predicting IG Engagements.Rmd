---
title: "NBA Hackathon, Business Analytics Question"
subtitle: "Tuned Random Forest Model for Predicting Instagram Engagments"
team name: "Lakers in 6"
authors: "Daniel Alpert, Nathaniel Hollenberg, Jefferey Huang, Ramtin Talebi"
output: pdf_document
---
\textbf{Introduction}

Presented with the task of predicting the number of engagements with Instagram posts of the \textit{@nba} account from the past two years, our team elected to create a partitioned Random Forest Model from the \textit{randomForest} package in \textit{R}. Our models were partitioned based on post medium (The prompt seemed to allude to this as well, so we proceeded accordingly). Here we present our process for cleaning and modifying the data, visualizing trends, training the Random Forest, and finally evaluating and tuning it.

First, we defined functions to calculate predicted values, absolute percent error of each prediction, and mean absolute percent error as follows (this will be useful for training later):

```{r}
## Create useful functions for calculating mape
Calc_Mape <- function(labels, dataset, model)
  {
    actual = labels
    predicted = predict(model, dataset)
    APE = abs((actual - predicted)/actual) * 100
    mape = mean(APE)
    return(mape)
}
Calc_Predicted <- function(model, dataset)
  {
    predicted = predict(model, dataset)
    return(predicted)
}
Calc_PE <- function(labels, dataset, model)
  
  {
    actual = labels
    predicted = predict(model, dataset)
    PE = ((actual - predicted)/actual) * 100
    return (PE)
}

```

Next we loaded in the necessary packages and read in the training set:

```{r}
## Load necessary libraries and read data
require(caTools)
require(gam)
require(xgboost)
require(Matrix)
library(readr)
library(stringr)
library(car)
library(dplyr)
library(ggplot2)
library(randomForest)

# Read in data
df <- read.csv("~/Desktop/NBA Analytics/Business Analytics/training_set_extended_5.csv")
```

\textbf{Predictors Preparation and Data Parsing}

After sorting and examining the dataset across a number of predictors, we noticed that some of the posts did not contain descriptions and others only contained an emoji or symbol. To avoid any unnecessary influence from these observations, we deleted these entries entirely (while we recognize that imputing the mean for the predictors of these observations is likely a better approach, we felt that this wouldn't entirely be accurate given that they were text descriptions).

In addition, we created variables for the number of instagram accounts mentioned (num_ats) and the number of hashtags (num_hash) in each post. We also converted some of our categorical predictors into numerical predictors (useful for creating a correlation plot later).

Note: In an attached Python file, you will find some code with our initial modifications to the dataset, including using a "bag-of-words" methodology to parse out important information from the Instagram descriptions.

```{r}
## Deleting missing data

# Order data by post description
df <- df[order(df$Description),]
head(df)

# Delete 
df <- df[-c(1:14), ]
df <- df[-c(6:21), ]
```

```{r}
## Create variables for number of people and number of hashtags
## mentioned, turn weekdays and post type into numbers
nba <- df %>% 
  mutate(num_ats = str_count(Description, "@"),
         num_hash = str_count(Description, "#"))

nba$Type_num <- NA
for(i in 1:length(nba[,1])){
  if(nba$Type[i] == "Video")(nba$Type_num[i] = 1)
  if(nba$Type[i] == "Photo")(nba$Type_num[i] = 2)
  if(nba$Type[i] == "Album")(nba$Type_num[i] = 3)
}
nba$wkdy_num <- NA
for(i in 1:length(nba[,1])){
  if(nba$weekday[i] == "Sunday")(nba$wkdy_num[i] = 1)
  if(nba$weekday[i] == "Monday")(nba$wkdy_num[i] = 2)
  if(nba$weekday[i] == "Tuesday")(nba$wkdy_num[i] = 3)
  if(nba$weekday[i] == "Wednesday")(nba$wkdy_num[i] = 4)
  if(nba$weekday[i] == "Thursday")(nba$wkdy_num[i] = 5)
  if(nba$weekday[i] == "Friday")(nba$wkdy_num[i] = 6)
  if(nba$weekday[i] == "Saturday")(nba$wkdy_num[i] = 7)
}
nba$day_time_num <- NA
for(i in 1:length(nba[,1])){
  if(nba$day_time[i] == "deadzone")(nba$day_time_num[i] = 1)
  if(nba$day_time[i] == "morning")(nba$day_time_num[i] = 2)
  if(nba$day_time[i] == "lunch")(nba$day_time_num[i] = 3)
  if(nba$day_time[i] == "afternoon")(nba$day_time_num[i] = 4)
  if(nba$day_time[i] == "evening")(nba$day_time_num[i] = 5)
  if(nba$day_time[i] == "night")(nba$day_time_num[i] = 6)
  if(nba$day_time[i] == "postgame")(nba$day_time_num[i] = 7)
}
```

```{r}
# Delete irrelevant indexing columns.
nba <- nba[,-c(1,2)]
```

\textbf{Data Visualizations}

Here we provide some visualizations of our training data to understand the relationships between some of our predictors and Engagements. First some boxplots of predictors effects on \textit{Engagements}.

```{r echo=FALSE}
nba$is_playoffs = as.factor(nba$is_playoffs)
nba$all_nba = as.factor(nba$all_nba)
nba$league_type = as.factor(nba$league_type)
nba$in_season = as.factor(nba$in_season)
nba$same_day_post = as.factor(nba$same_day_post)

g <- ggplot(nba, aes(x = is_playoffs, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#FFDB6D") + labs(title="Effect of Playoff Season on Engagements", x= "During Playoff Season", y="Engagements") + theme_minimal()

g <- ggplot(nba, aes(x = all_nba, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#C4961A") + labs(title="Effect of Star Players on Engagements", x= "Type of Star Players", y="Engagements") + theme_minimal()

g <- ggplot(nba, aes(x = league_type, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#F4EDCA") + labs(title="Effect of NBA vs GLeague vs WNBA on Engagements", x= "Type of League", y="Engagements") + theme_minimal()

g <- ggplot(nba, aes(x = in_season, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#D16103") + labs(title="Effect of Off-Season on Engagements", x= "In-Season", y="Engagements") + theme_minimal()

g <- ggplot(nba, aes(x = weekday, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#C3D7A4") + labs(title="Engagements by Day", x= "Weekday", y="Engagements") + scale_x_discrete(limits = c("Monday", "Tuesday", "Wednesday", "Thursday","Friday", "Saturday","Sunday")) + theme_minimal()

g <- ggplot(nba, aes(x = day_time, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#52854C") + labs(title="Engagements by Time of Day", x= "day_time", y="Engagements") + scale_x_discrete(limits = c("deadzone", "morning", "lunch", "afternoon","evening", "night","postgame")) + theme_minimal()

g <- ggplot(nba, aes(x = month, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#4E84C4") + labs(title="Engagements by Month", x= "Month", y="Engagements") + scale_x_discrete(limits = c("October", "November", "December","January", "February","March", "April","May","June","July","August","September")) + theme_minimal()

g <- ggplot(nba, aes(x = same_day_post, y = Engagements))
g + geom_boxplot(position = "dodge", fill = "#CC79A7") + labs(title="Effect of Posting in the Same Time Period", x= "Same Time Period", y="Engagements") + theme_minimal()
```

In order to assess multicollinearity in our predictors, we created a plot to visualize correlations between each variable The first plot displays the pearson correlation coefficient (R) between predictors, shown by the size and color of the circles at each point in the matrix. The second plot shows the same matrix, but this time with insignificant predictors (p-value < 0.01) crossed-out. With the remaining predictors, nothing in particular seems striking, except for the correlation between \textit{team_foll_ment} and \textit{num_@}. Intuitively this makes sense, a post must mention another team's IG account in order for the \textit{team_foll_ment} to be a non-zero number. It may be worth looking into removing this variable after some cross validation. 

```{r echo = FALSE}
library(corrplot)
library(Hmisc)

# Convert all predictors to numerical values
nba$Followers_Mentioned = as.numeric(nba$Followers_Mentioned)
nba$Followers = as.numeric(nba$Followers)
nba$is_playoffs = as.numeric(nba$is_playoffs)
nba$all_nba = as.numeric(nba$all_nba)
nba$league_type = as.numeric(nba$league_type)
nba$in_season = as.numeric(nba$in_season)
nba$same_day_post = as.numeric(nba$same_day_post)
nba$num_ats = as.numeric(nba$num_ats)
nba$num_hash = as.numeric(nba$num_hash)
nba$wkdy_num = as.numeric(nba$wkdy_num)
nba$month_int = as.numeric(nba$month_int)

# Create a plot correlation matrix
nbavars <- nba[ ,c(2,9,12,14:20,22)]
res <- cor(nbavars)
res2 <- rcorr(as.matrix(nbavars))
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, title = "R values between predictors")
corrplot(res2$r, type="upper", order="hclust", tl.col = "black", p.mat = res2$P, sig.level = 0.01, insig = "pch",tl.srt = 45)

```

Lastly, as hinted by the prompt, we viewed the distributions of our training data by post type (either Video, Album, or Photo) to determine whether they differed significantly.

While the distribution of Video and Photo distribution seems to be fairly normal, Album appears bimodal, and all three fall under different mean values for Engagements. Based on the plots, it does seem like each distribution is different enough to warrant each having their own model. In addition, the data set contains many more videos than it does photos or albums, so if we chose to simply include post type as a predictor in one model, we'd have to weight each level accordingly. Instead we chose to create three seperate models.

```{r echo=FALSE}
# Density Plot of Engagements Post Type
library(ggplot2)
ggplot(nba) +
  geom_density(aes(x=Engagements, fill=Type), 
                 colour="grey50", alpha=0.5, position="identity") + ylab("Density") + ggtitle("Distribution of Engagements by Post Type")

```

\textbf{Model Training and Validation}

First we scramble our training set in preparation for building our model.

```{r}
## Subset data for just videos and scramble data
set.seed(9212)
nba <- nba[sample(1:nrow(nba)), ]
```

Although the code is not included here, we tried to create a number of different models, inlcuding a least squares regression model, a gradient boosted model, and even using predicted values from these models into other models. Ultimately, after testing the accuracy of each, we settled on a Random Forest, as it appeared to perform best and was the least overfitted.

In order to train and validate our Random Forest model, we first divide our training set by post type, and then within each division, we subset a small testing data set from our full training set and use it to calculate the MAPE. More extensive cross-validation methods were used as well, but for brevity's sake, we don't include them here.

```{r results='hide'}
# Creating Video/Photo/Album subsets from nba data

nba_Video = nba[nba$Type_num == 1,]
nba_Album = nba[nba$Type_num == 2,]
nba_Photo = nba[nba$Type_num == 3,]

# Splitting Video/Album/Photo subsets into training and test sets
numberOfTrainingSamples_Video <- round(length(nba_Video$Engagements) * .80)
numberOfTrainingSamples_Photo <- round(length(nba_Photo$Engagements) * .80)
numberOfTrainingSamples_Album <- round(length(nba_Album$Engagements) * .80)

train_dataRF_Video <- nba_Video[1:numberOfTrainingSamples_Video,]
test_dataRF_Video <- nba_Video[-(1:numberOfTrainingSamples_Video),]

train_dataRF_Album<- nba_Album[1:numberOfTrainingSamples_Album,]
test_dataRF_Album <- nba_Album[-(1:numberOfTrainingSamples_Album),]

train_dataRF_Photo <- nba_Photo[1:numberOfTrainingSamples_Photo,]
test_dataRF_Photo <- nba_Photo[-(1:numberOfTrainingSamples_Photo),]
```

```{r}
#varsVid = colnames(train_dataRF_Video[c(2,8,10,11,12,14,15,16,17,18,19,20)])
#varsPhoto = colnames(train_dataRF_Photo[c(2,8,10,11,12,14,15,16,17,18,19,20)])
#varsAlbum = colnames(train_dataRF_Album[c(2,8,10,11,12,14,15,16,17,18,19,20)])
```

We then train the Random Forest model to predict \textit{Engagements} on the test set and then calculate and append columns for predicted Engagements (pred_rf), percent error (PE_rf), and absolute percent error (APE_rf). 

```{r}
# Models for each subset
RF_Video <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, xtest = test_dataRF_Video[,c(2,8,10,11,12,14:20)], ytest = test_dataRF_Video$Engagements, ntree=1000, mtry=8, maxnodes=100,importance=TRUE, data=train_dataRF_Video)
RF_Video

RF_Photo <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, xtest = test_dataRF_Photo[,c(2,8,10,11,12,14:20)], ytest = test_dataRF_Photo$Engagements, ntree=1000, mtry=9, maxnodes=100,importance=TRUE, data=train_dataRF_Photo)
RF_Photo

RF_Album <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, xtest = test_dataRF_Album[,c(2,8,10,11,12,14:20)], ytest = test_dataRF_Album$Engagements, ntree=1000, mtry=8, maxnodes=100,importance=TRUE, data=train_dataRF_Album)
RF_Album
```

```{r echo=FALSE}
# Calculating MAPE of training sets
RF_Video_for_calc <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=8, maxnodes=100,importance=TRUE, data=train_dataRF_Video)

RF_Photo_for_calc <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=7, maxnodes=100,importance=TRUE, data=train_dataRF_Photo)

RF_Album_for_calc <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=7, maxnodes=100,importance=TRUE, data=train_dataRF_Album)

test_dataRF_Video$predicted = Calc_Predicted(RF_Video_for_calc, test_dataRF_Video)
test_dataRF_Album$predicted = Calc_Predicted(RF_Album_for_calc, test_dataRF_Album)
test_dataRF_Photo$predicted = Calc_Predicted(RF_Photo_for_calc, test_dataRF_Photo)

mape_Rf_Video = mean((abs(test_dataRF_Video$Engagements - test_dataRF_Video$predicted)/test_dataRF_Video$Engagements))*100
mape_Rf_Album = mean((abs(test_dataRF_Album$Engagements - test_dataRF_Album$predicted)/test_dataRF_Album$Engagements))*100
mape_Rf_Photo = mean((abs(test_dataRF_Photo$Engagements - test_dataRF_Photo$predicted)/test_dataRF_Photo$Engagements))*100

test_data_net = rbind(test_dataRF_Video, test_dataRF_Album, test_dataRF_Photo)

test_data_net$percent_error = ((test_data_net$Engagements - test_data_net$predicted)/test_data_net$Engagements)*100
test_data_net$APE = abs(test_data_net$percent_error)

mape_Rf = mean((abs(test_data_net$Engagements - test_data_net$predicted)/test_data_net$Engagements))*100
```

```{r}
mape_Rf
```
Our MAPE came out to about 5.5%. Not bad!

Let's take a look at how our percent error is distributed across the testing set.

```{r}
# Visualization of percent error of our model's predicted values

hist(test_data_net$percent_error, breaks=50, col = 'red')
```
It appears that there are a few outliers in our dataset that the model performs particularly poorly on. But given how few they are, and that the Random Forest is typically robust to outliers, we conclude that our model performs quite well.

```{r}
#cvVideo = rfcv(train_dataRF_Video[,c(2,8,10,11,12,14:20)],train_dataRF_Video$Engagements,step=0.9)
#cvAlbum = rfcv(train_dataRF_Album[,c(2,8,10,11,12,14:20)],train_dataRF_Album$Engagements,step=0.9)
#cvPhoto = rfcv(train_dataRF_Photo[,c(2,8,10,11,12,14:20)],train_dataRF_Photo$Engagements,step=0.9)
#cbind(nvars=cvVideo$n.var, error.rate=cvVideo$error.cv) 
#cbind(nvars=cvAlbum$n.var, error.rate=cvAlbum$error.cv) 
#cbind(nvars=cvPhoto$n.var, error.rate=cvPhoto$error.cv) 
```

Here we plot some of our variables to examine which are most critical to our model's accuracy. Notably, it appears that our variables vary in importance between the subsets for Videos, Albums, and Photos.

```{r}
varImpPlot(RF_Video)
varImpPlot(RF_Album)
varImpPlot(RF_Photo)
```

Finally, statisfied with our validation results, we train our Random Forest on the whole of the training set and apply it to the 1000-post holdout set.

```{r}
#holdout <- read.csv("~/Desktop/NBA Analytics/Business Analytics/holdout_set.csv")

#write.xlsx(holdout, file = "holdout_set.xlsx")

## Models for training and prediction

#RF_Video_Final <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=8, maxnodes=100,importance=TRUE, data=nba_Video)

#RF_Photo_Final <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=9, maxnodes=100,importance=TRUE, data=nba_Photo)

#RF_Album_Final <- randomForest(Engagements ~ Followers+month+weekday+day_time+is_playoffs+Followers_Mentioned+all_nba+league_type+in_season+same_day_post+num_ats+num_hash, ntree=1000, mtry=8, maxnodes=100,importance=TRUE, data=nba_Album)
```

```{r}
#Calculate APE and append to holdout dataframe
#for (i in holdout)
 # {
   # if (holdout$Type_num[i] == 1)(holdout$Engagements[i] = Calc_Predicted(RF_Video_Final, holdout))
 #   if (holdout$Type_num[i] == 2)(holdout$Engagements[i] = Calc_Predicted(RF_Photo_Final, holdout))
   # if (holdout$Type_num[i] == 3)(holdout$Engagements[i] = Calc_Predicted(RF_Album_Final, holdout))
#}

#head(holdout)
```

The predictions are contained in the attached "holdout.csv" file.

